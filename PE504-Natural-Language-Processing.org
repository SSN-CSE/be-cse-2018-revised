* <<<PE504>>> NATURAL LANGUAGE PROCESSING
:properties:
:author: Dr. D. THenmozhi and Mr. B. Senthil Kumar
:date: 
:end:

#+begin_comment
- 1. Combined Unit 2 and 3 of AU into Unit 2, Unit 4 and 5 of AU into Unit 3 to give emphasis on
   NLP applications
- 2. For changes, see the indidual units
- 3. The unit headings are similar to M.E syllabus with addition and deletion of topics except Unit 4. 
     Unit 4 and 5 are focussing on NLP applications. Removed NLP using Python
- 4. Five Course outcomes specified and aligned with units
- 5. Not Applicable
#+end_comment

#+startup: showall

{{{credits}}}
|L|T|P|C|
|3|0|0|3|

** COURSE OBJECTIVES
- To learn language models
- To understand the levels of knowledge in language processing
- To develop NLP applications.

{{{unit}}}
| UNIT I | OVERVIEW AND LANGUAGE MODELING | 8 |
Origins and challenges of NLP -- Knowledge in language processing --
NLP applications; Language Modeling: Language and grammar --
Grammar-based language models -- Lexical functional grammar --
Government and binding; Statistical Language Model: N-gram model --
Smoothing techniques.

#+begin_comment

- 1. In AU syllabus few topics of word level analysis are is covered in this unit. We have moved those topics to Unit 2. 
- 2. Smoothing techniques are part of statistical language models are given in Unit 2. We have included in Unit 1. 
- 3. Removed spelling error detection and correction techniques.
- 4. Unit 1 is same as Unit 1 of M.E syllabus
#+end_comment

{{{unit}}}
| UNIT II | WORD LEVEL AND SYNTACTIC ANALYSIS | 10 |
Word Level Analysis: Regular expressions -- Survey of morphology --
Word and sentence tokenization -- Stemmer -- Word classes --
Part-of-Speech Tagging: HMM POS tagging; Syntactic Analysis:
Constituency -- Context-free grammar -- Dependency Grammar; Parsing:
Top-down -- Bottom-up -- Ambiguity -- Early algorithm -- CYK --
Probabilistic CFG -- Probabilistic CYK parsing; Tree banks.

#+begin_comment

- 1. As Unit 2 and 3 of AU are combined to Unit 2, the topics namely Transducers, Maximum Entropy models, Probabilistic Lexicalized CFGs, Feature structures and Unification of feature structures are removed.
- 2. Unit 2 is same as Unit 2 of M.E syllabus
#+end_comment


{{{unit}}}
| UNIT III | SEMANTIC ANALYSIS | 9 |
The representation of Meaning: Meaning representation -- Computational desiderata for representation; 
Lexical Semantics: Word senses -- Relations -- WordNet -- Thematic roles -- Selectional restrictions; 
Word Sense Disambiguation: Dictionary-based -- Supervised -- Minimally-supervised -- Unsupervised; 
Word Similarity: Thesaurus methods -- Distributional methods.

#+begin_comment

- 1. The topics namely First-Order Logic and Description Logics are removed. Unsupervised WSD is added
- 2. Discourse processing is moved to Unit 4 when compared with Unit 3 of M.E syllabus
#+end_comment

{{{unit}}}
| UNIT IV | DISCOURSE PROCESSING, IR AND IE | 9 |
Discourse Processing: Reference resolution -- Anaphora resolution algorithms -- Co-reference resolution; 
Information Retrieval: The vector space model -- Term weighting -- Evaluation of IR; 
Information Extraction: Named entity recognition -- Relation detection and classification.

#+begin_comment

- 1. AU does not focus on NLP applications. NLP applications namely IR and IE are included in this unit. Discourse processing of Unit 5 of AU syllabus is included here.
- 2. Removed Unit 4 of M.E syllabus and included applications in Unit 4 and 5 in detail.
#+end_comment

{{{unit}}}
| UNIT V | MACHINE TRANSLATION AND QUESTION ANSWERING  | 9 |
Machine Translation(MT): Problems in machine translation -- Classical
MT -- Statistical MT; Factoid Question Answering: Question processing -- Passage retrieval
-- Answer processing -- Evaluation of factoid answers.

#+begin_comment

- 1. AU focuses on lexical resources but not on applications. NLP applications namely MT and QA  are included in this unit. Lexical resources are removed which will be indirectly covered through NLP applications.
- 2. Additionally included the application namely QA when compared with M.E syllabus.
#+end_comment

\hfill *Total Periods: 45*

** COURSE OUTCOMES
Upon the completion of the course the students should be able to: 
- Describe the language models (K3)
- Explain levels of knowledge in language processing (K3)
- Apply computational methods in semantic and discourse processing (K3)
- Apply NLP techniques to MT, IR, IE, QA and Summarization systems (K2)
- Apply evaluation metrics for different NLP applications (K3).

** TEXT BOOKS
1. Daniel Jurafsky and James H Martin, ``Speech and Language
   Processing: An introduction to Natural Language Processing,
   Computational Linguistics and Speech Recognition'', 2nd Edition,
   Prentice Hall, 2008.
2. Tanveer Siddiqui, U S Tiwary, ``Natural Language Processing and
   Information Retrieval'', Oxford University Press, 2008.

** REFERENCES
1. Christopher D Manning, Hinrich Schutze, ``Foundations of
   Statistical Natural Language Processing'', MIT Press, 1999.
2. Nitin Indurkhya, Fred J Damerau, ``Handbook of Natural Language
   Processing'', 2nd Edition, CRC Press, 2010.
3. Steven Bird, Ewan Klein, ``Natural Language Processing with
   Python'', O'Reilly Media, 2009.
4. Ruslan Mitkov, ``The Oxford Handbook of Computational
   Linguistics'', Oxford University Press, 2009.
5. NLTK -- Natural Language Tool Kit - http://www.nltk.org/.
